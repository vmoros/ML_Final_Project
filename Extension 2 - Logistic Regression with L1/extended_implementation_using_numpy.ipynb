{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Homemade implementations of Extension 2 - Logistic Regression with L1 regularization</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the dogs data\n",
    "\n",
    "I took the Chihuahua and Japanese Spaniel images and put them in the \"selected_images/class0\" directory and I put the Maltese and Pekinese images in the \"selected_images/class1\" directory.\n",
    "\n",
    "So the classifier is distinguishing between \\[Chihuahua or Japanese Spaniel\\] and \\[Maltese or Pekinese\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_data():\n",
    "    pics = list()\n",
    "    y_all = np.array([], dtype=np.int8)\n",
    "    new_size = (6, 6)\n",
    "\n",
    "    for i, folder in enumerate([\"selected_images/class0\", \"selected_images/class1\"]):\n",
    "        for f in glob.glob(f\"{folder}/*\"):\n",
    "            curr_pic = np.array(Image.open(f).resize(new_size)).reshape(-1) # shrink and flatten the pic\n",
    "            pics.append(curr_pic)\n",
    "            y_all = np.concatenate((y_all, [i])) # all pictures in a given folder have the same label\n",
    "            \n",
    "    X_all = np.array(pics)\n",
    "    return X_all, y_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ones(m):\n",
    "    ones = np.ones(m.shape[0]).reshape(-1, 1)\n",
    "    return np.hstack((ones, m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all, y_all = get_all_data()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=0.8, random_state=0)\n",
    "X_train = StandardScaler().fit_transform(X_train)\n",
    "X_train = add_ones(X_train)\n",
    "X_test = StandardScaler().fit_transform(X_test)\n",
    "X_test = add_ones(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_bc, y_all_bc = load_breast_cancer(return_X_y=True)\n",
    "X_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X_all_bc, y_all_bc, train_size=0.8, random_state=0)\n",
    "scaler = StandardScaler().fit(X_train_bc)\n",
    "X_train_bc = scaler.transform(X_train_bc)\n",
    "X_train_bc = add_ones(X_train_bc)\n",
    "X_test_bc = scaler.transform(X_test_bc)\n",
    "X_test_bc = add_ones(X_test_bc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homemade implementation of logistic regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def hypothesis(X , w):\n",
    "    return sigmoid(X @ w)\n",
    "\n",
    "classify = np.vectorize(lambda x: 1 if x >= 0.5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L1_grad(w): \n",
    "    grad = np.sign(w)\n",
    "    grad[0][0] = 0     # don't penalize the intercept term\n",
    "    return grad\n",
    "\n",
    "def Logistic_Regression_Gradient_Ascent(X, y, learning_rate, reg_rate, num_iters, reg):\n",
    "    log_likelihood_values = []\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    N = X.shape[0]\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        h = hypothesis(X, w)\n",
    "        w += (learning_rate / N) * (X.T @ (y - h))\n",
    "        if reg:\n",
    "            new_w = w - (learning_rate * reg_rate / N) * L1_grad(w)\n",
    "            new_w[np.sign(new_w) != np.sign(w)] = 0 # clipped L1 from page 479 of the paper\n",
    "            w = new_w\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_LR_f1(train_size, X_all, y_all):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=train_size, random_state=0)\n",
    "    scaler = StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    plain_w = Logistic_Regression_Gradient_Ascent(X_train, y_train, 0.1, 0, 1000, False)\n",
    "    plain_preds = classify(hypothesis(X_test, plain_w))    \n",
    "    L1_w = Logistic_Regression_Gradient_Ascent(X_train, y_train, 0.1, 10, 1000, True)\n",
    "    L1_preds = classify(hypothesis(X_test, L1_w))\n",
    "    \n",
    "    return 100 * f1_score(y_test, plain_preds), 100 * f1_score(y_test, L1_preds)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate my homemade implementation's F1 scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### i) Check the F1 score of my logistic regression with L1 regularization on the dogs data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score of my LR with L1 on the dog data is 64.86486486486486\n"
     ]
    }
   ],
   "source": [
    "plain_f1, l1_f1 = get_LR_f1(0.8, X_all, y_all)\n",
    "print(f\"The F1 score of my LR with L1 on the dog data is {l1_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ii) Check the F1 score of my logistic regression with L1 regularization on the breast cancer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The F1 score of my LR with L1 on the breast cancer data is 97.10144927536231\n"
     ]
    }
   ],
   "source": [
    "l1_w_bc = Logistic_Regression_Gradient_Ascent(X_train_bc, y_train_bc, 0.1, 10, 1000, True)\n",
    "l1_preds_bc = classify(hypothesis(X_test_bc, l1_w_bc))\n",
    "print(f\"The F1 score of my LR with L1 on the breast cancer data is {100 * f1_score(y_test_bc, l1_preds_bc)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
